<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yucheng Lu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Home</h1>
</div>
<table class="imgtable"><tr><td>
<img src="self2.jpeg" alt="yucheng" width="300px"/>&nbsp;</td>
<td align="left"><p>Yucheng Lu (陆昱成)<br />
Email: yl2967 [at] cornell [dot] edu <br />
<a href="https://scholar.google.com/citations?user=FsBgPhQAAAAJ&hl=en">Google Scholar</a> / <a href="https://twitter.com/yuchengthekid">Twitter</a>
</td></tr></table>
<h2>Short Bio</h2>
<p>Yucheng Lu is an AI Engineer at <a href="https://together.ai/">Together</a>. He obtained his Ph.D. degree at <a href="https://www.cs.cornell.edu/">Cornell Computer Science</a>,
where he was advised by <a href="https://www.cs.cornell.edu/~cdesa/">Prof. Chris De Sa</a>. 
Yucheng is broadly interested in building scalable, provably efficient and ubiquitous deep learning systems.
His projects have touched on model compression, communication compression, decentralized/distributed ML, sample ordering, etc.</p>
<p>Yucheng's research has been recognized by <a href="https://icml.cc/virtual/2021/awards_detail">ICML Outstanding Paper Award (Honorable Mention)</a>
and <a href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">Meta PhD Fellowship</a>.
He has also worked/interned at <a href="https://www.microsoft.com/en-us/research/project/deepspeed/">Microsoft DeepSpeed</a>, 
<a href="https://research.google/">Google Cerebra</a>
and <a href="https://aws.amazon.com/forecast/">Amazon Forecast</a>.</p>
<p>Yucheng obtained his BEng degree in <a href="http://english.seiee.sjtu.edu.cn/">Electronic Engineering</a> from <a href="http://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>. </p>
<h2>Updates</h2>
<p>[Apr&rsquo;23] <a href="CocktailSGD">CocktailSGD</a> is accepted by ICML&rsquo;23, we evaluate empirically the effect of communication compression on distributed LLM finetuning!</p>
<p>[Apr&rsquo;23] <a href="https://arxiv.org/abs/2302.01172">STEP</a> is accepted by ICML&rsquo;23, we propose an Adam-aware recipe for learning N:M strucutred sparsity masks on LLMs!</p>
<p>[Jan&rsquo;23] <a href="https://arxiv.org/abs/2202.06009">0/1 Adam</a> is accepted by ICLR&rsquo;23, we propose an Adam variant to accelerate LLM pretraining in distributed systems!</p>
<p>[Oct&rsquo;22] NeurIPS&rsquo;22 Scholar Award, thanks!</p>
<p>[Sep&rsquo;22] <a href="https://arxiv.org/abs/2205.10733">GraB</a> is accepted by NeurIPS&rsquo;22, we propose algorithms to construct provably better data permutations than random reshuffling!</p>
<p>[Feb&rsquo;22] Won <a href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">Meta PhD Fellowship 2022</a>, thanks Meta!</p>
<p>[Jan&rsquo;22] <a href="https://openreview.net/pdf?id=7gWSJrP3opB">QMC-Example-Selection</a> is accepted by ICLR&rsquo;22 as spotlight (5%), we analyzed the complexity for example selection and proposed two related algorithms!</p>
<p>[Oct&rsquo;21] <a href="https://nips.cc/Conferences/2021/ProgramCommittee">Outstanding Reviewer Award</a> (8%) at NeurIPS&rsquo;21!</p>
<p>[Sep&rsquo;21] <a href="https://arxiv.org/abs/2102.03034">HyperDeception</a> is accepted by NeurIPS&rsquo;21, we studied justifiable hyperparameter optimization via modal logic!</p>
<p>[Jul&rsquo;21] <a href="http://proceedings.mlr.press/v139/lu21a.html">DeTAG</a> won <a href="https://icml.cc/virtual/2021/awards_detail">Outstanding Paper Award Honorable Mention</a> at ICML&rsquo;21  (5 out of 5513 submissions)!</p>
<p>[May&rsquo;21] <a href="http://proceedings.mlr.press/v139/lu21a.html">DeTAG</a> is accepted by ICML&rsquo;21 as Long Oral (3%), we discussed the theoretical limits of decentralized training, and how to achieve it!</p>
<p>[May&rsquo;21] <a href="http://proceedings.mlr.press/v139/lu21d.html">SCott</a> is accepted by ICML&rsquo;21, we discussed how to use stratification in training forecasting models!</p>
<p>[May&rsquo;20] <a href="http://proceedings.mlr.press/v119/lu20a.html">Moniqua</a> is accepted by ICML&rsquo;20, we discussed how to compress communication in learning systems without additional memory!</p>
<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
