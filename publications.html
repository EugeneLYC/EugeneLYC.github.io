<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications and Manuscripts</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">About</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="lab.html">Lab</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications and Manuscripts</h1>
</div>
<p>(* denotes equal contribution)</p>
<h2>2023</h2>
<ul>
<li><p><b>Provably Efficient Model Training over Centralized and Decentralized Datasets</b><br />
<b>Yucheng Lu</b> <br />
[<a href="https://ecommons.cornell.edu/items/ecb07244-8512-4c97-8997-1273a077d92c">PhD Thesis</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Coordinating Distributed Example Orders for Provably Accelerated Training</b><br />
A. Feder Cooper*, Wentao Guo*, Khiem Pham*, Tiancheng Yuan, Charlie Ruan, <b>Yucheng Lu</b>, Chris De Sa<br />
<i> In Proceedings of the 36th Neural Information Processing Systems Conference (NeurIPS) 2023. </i> <br />
[<a href="https://openreview.net/forum?id=ISRyILhAyS">Proceedings</a>][<a href="https://arxiv.org/abs/2302.00845">Arxiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks</b><br />
Jue Wang*, <b>Yucheng Lu</b>*, Binhang Yuan, Beidi Chen, Percy Liang, Chris De Sa, Chris RÃ©, Ce Zhang<br />
<i> In the Fortieth International Conference on Machine Learning (ICML) 2023. </i> <br />
[<a href="https://openreview.net/pdf?id=w2Vrl0zlzA">Proceedings</a>]</p>
</li>
</ul>
<ul>
<li><p><b>STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition</b><br />
<b>Yucheng Lu</b>, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Chris De Sa, Amir Yazdanbakhsh<br />
<i> In the Fortieth International Conference on Machine Learning (ICML) 2023. </i> <br />
[<a href="https://proceedings.mlr.press/v202/lu23c/lu23c.pdf">Proceedings</a>][<a href="https://arxiv.org/abs/2302.01172">Arxiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam</b><br />
<b>Yucheng Lu</b>, Conglong Li, Minjia Zhang, Chris De Sa, Yuxiong He<br />
<i> In the Eleventh International Conference on Learning Representations (ICLR) 2023. </i> <br />
[<a href="https://arxiv.org/abs/2202.06009">Arxiv</a>][<a href="https://www.deepspeed.ai/tutorials/zero-one-adam/">Tutorial</a>][<a href="https://github.com/microsoft/DeepSpeed">Code</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Decentralized Learning: Theoretical Optimality and Practical Improvements</b><br />
<b>Yucheng Lu</b>, Chris De Sa<br />
<i> In the Journal of Machine Learning Research (JMLR) 2023. </i> <br />
[<a href="https://www.jmlr.org/papers/volume24/22-0044/22-0044.pdf">JMLR</a>] <br /></p>
</li>
</ul>
<h2>2022</h2>
<ul>
<li><p><b>GraB: Finding Provably Better Data Permutations than Random Reshuffling</b><br />
<b>Yucheng Lu</b>, Wentao Guo, Chris De Sa<br />
<i> In the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022. </i> <br />
[<a href="https://openreview.net/pdf?id=nDemfqKHTpK">Proceedings</a>][<a href="https://arxiv.org/abs/2205.10733">Arxiv</a>][<a href="https://github.com/EugeneLYC/GraB">Code</a>]</p>
</li>
</ul>
<ul>
<li><p><b>A General Analysis of Example-Selection for Stochastic Gradient Descent</b><br />
<b>Yucheng Lu</b>*, Si Yi Meng*, Chris De Sa<br />
<i> In the Tenth International Conference on Learning Representations (ICLR) 2022. </i> <strong style="color: red;">Spotlight</strong> <br />
[<a href="https://openreview.net/pdf?id=7gWSJrP3opB">Proceedings</a>][<a href="https://github.com/EugeneLYC/qmc-ordering">Code</a>]</p>
</li>
</ul>
<h2>2021</h2>
<ul>
<li><p><b>Hyperparameter Optimization is Deceiving Us, and How to Stop It</b><br />
A. Feder Cooper, <b>Yucheng Lu</b>, Jessica Zosa Forde, Chris De Sa<br />
<i> In the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) 2021. </i> <br />
[<a href="https://proceedings.neurips.cc/paper/2021/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html">Proceedings</a>][<a href="https://arxiv.org/abs/2102.03034">Arxiv</a>][<a href="https://github.com/pasta41/deception">Code</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Variance Reduced Training with Stratified Sampling for Forecasting Models</b><br />
<b>Yucheng Lu</b>, Youngsuk Park, Lifan Chen, Yuyang Wang, Chris De Sa, Dean Foster<br />
<i> In the Thirty-eighth International Conference on Machine Learning (ICML) 2021. </i> <br />
[<a href="http://proceedings.mlr.press/v139/lu21d.html">Proceedings</a>][<a href="https://arxiv.org/abs/2103.02062">Arxiv</a>][<a href="https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/nursery">Code</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Optimal Complexity in Decentralized Training</b><br />
<b>Yucheng Lu</b>, Chris De Sa<br />
<i> In the Thirty-eighth International Conference on Machine Learning (ICML) 2021. </i> <strong style="color: red;"> Outstanding Paper Award Honorable Mention</strong> <br />
[<a href="http://proceedings.mlr.press/v139/lu21a.html">Proceedings</a>][<a href="https://arxiv.org/abs/2006.08085">Arxiv</a>][<a href="./files/DeTAG_errata.pdf">Errata</a>][<a href="https://www.leiphone.com/category/academic/ttPXXZRVE2IgyJfj.html">Media Coverage (Chinese)</a>] <br /></p>
</li>
</ul>
<h2>2020</h2>
<ul>
<li><p><b>MixML: A Unified Analysis of Weakly Consistent Parallel Learning</b><br />
<b>Yucheng Lu</b>, Jack Nash, Chris De Sa<br />
<i> Unpublished Manuscript </i> <br />
[<a href="https://arxiv.org/abs/2005.06706">Arxiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Adaptive Diffusion of Sensitive Information In Online Social Networks</b><br />
Xudong Wu, Luoyi Fu, Huan Long, Dali Yang, <b>Yucheng Lu</b>, Xinbing Wang, Guihai Chen<br />
<i> In IEEE Transactions on Knowledge and Data Engineering (TKDE) 2020. </i> <br />
[<a href="https://ieeexplore.ieee.org/abstract/document/8950034">Paper</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Moniqua: Modulo Quantized Communication in Decentralized SGD</b><br />
<b>Yucheng Lu</b>, Chris De Sa<br />
<i> In the Thirty-seventh International Conference on Machine Learning (ICML) 2020. </i> <br />
[<a href="http://proceedings.mlr.press/v119/lu20a.html">Proceedings</a>][<a href="https://arxiv.org/abs/2002.11787">Arxiv</a>]</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
