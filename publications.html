<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications and Manuscripts</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yucheng Lu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications and Manuscripts</h1>
</div>
<h2>Preprints</h2>
<p><b>CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training</b><br />
<a href="https://cacioepe.pe/">A. Feder Cooper</a>*, Wentao Guo*, Khiem Pham*, Tiancheng Yuan, Charlie Ruan, Yucheng Lu, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
[<a href="https://arxiv.org/abs/2302.00845">Arxiv</a>]</p>
<h2>Publications</h2>
<p>(* denotes equal contribution)</p>
<p><b>CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks</b><br />
<a href="https://juewang.me/about/index.html">Jue Wang</a>*, Yucheng Lu*, <a href="https://binhangyuan.github.io/site/">Binhang Yuan</a>, <a href="https://www.andrew.cmu.edu/user/beidic/">Beidi Chen</a>, <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a>, <a href="https://cs.stanford.edu/~chrismre/">Christopher Re</a>, <a href="https://zhangce.github.io/">Ce Zhang</a><br />
<i> In the Fortieth International Conference on Machine Learning (ICML) 2023. </i> <br />
To Appear</p>
<p><b>STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition</b><br />
Yucheng Lu, Shivani Agrawal, <a href="http://people.csail.mit.edu/suvinay/">Suvinay Subramanian</a>, Oleg Rybakov, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a>, <a href="https://www.ayazdan.com/">Amir Yazdanbakhsh</a><br />
<i> In the Fortieth International Conference on Machine Learning (ICML) 2023. </i> <br />
To Appear</p>
<p><b>Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam</b><br />
Yucheng Lu, <a href="https://conglongli.github.io/">Conglong Li</a>, <a href="http://zhangminjia.me/">Minjia Zhang</a>, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a>, 
<a href="https://www.microsoft.com/en-us/research/people/yuxhe/">Yuxiong He</a><br />
<i> In the Eleventh International Conference on Learning Representations (ICLR) 2023. </i> <br />
[<a href="https://arxiv.org/abs/2202.06009">Arxiv</a>][<a href="https://www.deepspeed.ai/tutorials/zero-one-adam/">Tutorial</a>][<a href="https://github.com/microsoft/DeepSpeed">Code</a>]</p>
<p><b>GraB: Finding Provably Better Data Permutations than Random Reshuffling</b><br />
Yucheng Lu, Wentao Guo, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
<i> In the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022. </i> <br />
[<a href="https://openreview.net/pdf?id=nDemfqKHTpK">Proceedings</a>][<a href="https://arxiv.org/abs/2205.10733">Arxiv</a>][<a href="https://github.com/EugeneLYC/GraB">Code</a>]</p>
<p><b>A General Analysis of Example-Selection for Stochastic Gradient Descent</b><br />
Yucheng Lu*, <a href="https://www.cs.cornell.edu/~siyimeng/">Si Yi Meng</a>*, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
<i> In the Tenth International Conference on Learning Representations (ICLR) 2022. </i> <br />
[<a href="https://openreview.net/pdf?id=7gWSJrP3opB">Proceedings</a>][<a href="https://github.com/EugeneLYC/qmc-ordering">Code</a>]  <strong style="color: red;">Spotlight (5%)</strong></p>
<p><b>Hyperparameter Optimization is Deceiving Us, and How to Stop It</b><br />
<a href="https://cacioepe.pe/">A. Feder Cooper</a>, Yucheng Lu, <a href="https://jzf2101.github.io/">Jessica Zosa Forde</a>, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
<i> In the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) 2021. </i> <br />
[<a href="https://proceedings.neurips.cc/paper/2021/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html">Proceedings</a>][<a href="https://arxiv.org/abs/2102.03034">Arxiv</a>][<a href="https://github.com/pasta41/deception">Code</a>]</p>
<p><b>Variance Reduced Training with Stratified Sampling for Forecasting Models</b><br />
Yucheng Lu, <a href="https://youngsuk0723.github.io/">Youngsuk Park</a>, Lifan Chen, <a href="http://www.mit.edu/~ywang02/">Yuyang Wang</a>, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a>, <a href="http://deanfoster.net/">Dean Foster</a><br />
<i> In the Thirty-eighth International Conference on Machine Learning (ICML) 2021. </i> <br />
[<a href="http://proceedings.mlr.press/v139/lu21d.html">Proceedings</a>][<a href="https://arxiv.org/abs/2103.02062">Arxiv</a>][<a href="https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/nursery">Code</a>]</p>
<p><b>Optimal Complexity in Decentralized Training</b><br />
Yucheng Lu, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
<i> In the Thirty-eighth International Conference on Machine Learning (ICML) 2021. </i> <br />
Longer version available in <i> the Journal of Machine Learning Research (JMLR) </i>. <br />
[<a href="http://proceedings.mlr.press/v139/lu21a.html">Proceedings</a>][<a href="https://arxiv.org/abs/2006.08085">Arxiv</a>][<a href="https://www.jmlr.org/papers/volume24/22-0044/22-0044.pdf">JMLR</a>][<a href="./files/DeTAG_errata.pdf">Errata</a>][<a href="https://www.leiphone.com/category/academic/ttPXXZRVE2IgyJfj.html">Media Coverage (Chinese)</a>]<strong style="color: red;"> Outstanding Paper Award Honorable Mention</strong> <br /></p>
<p><b>MixML: A Unified Analysis of Weakly Consistent Parallel Learning</b><br />
Yucheng Lu, Jack Nash, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
<i> Unpublished Manuscript </i> <br />
[<a href="https://arxiv.org/abs/2005.06706">Arxiv</a>]</p>
<p><b>Adaptive Diffusion of Sensitive Information In Online Social Networks</b><br />
Xudong Wu, <a href="http://www.cs.sjtu.edu.cn/~fu-ly/index.html">Luoyi Fu</a>, <a href="http://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=269">Huan Long</a>, Dali Yang, Yucheng Lu, <a href="http://www.cs.sjtu.edu.cn/~wang-xb/">Xinbing Wang</a>, <a href="http://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=180">Guihai Chen</a><br />
<i> In IEEE Transactions on Knowledge and Data Engineering (TKDE) 2020. </i> <br />
[<a href="https://ieeexplore.ieee.org/abstract/document/8950034">Paper</a>]</p>
<p><b>Moniqua: Modulo Quantized Communication in Decentralized SGD</b><br />
Yucheng Lu, <a href="http://www.cs.cornell.edu/~cdesa/">Christopher De Sa</a><br />
<i> In the Thirty-seventh International Conference on Machine Learning (ICML) 2020. </i> <br />
[<a href="http://proceedings.mlr.press/v119/lu20a.html">Proceedings</a>][<a href="https://arxiv.org/abs/2002.11787">Arxiv</a>]</p>
<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
