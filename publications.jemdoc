# jemdoc: menu{MENU}{publications.html}
= Publications and Manuscripts

(* denotes equal contribution)


== 2023

- *Provably Efficient Model Training over Centralized and Decentralized Datasets*\n
*Yucheng Lu* \n
\[[https://ecommons.cornell.edu/items/ecb07244-8512-4c97-8997-1273a077d92c PhD Thesis]\]

- *Coordinating Distributed Example Orders for Provably Accelerated Training*\n
A. Feder Cooper\*, Wentao Guo\*, Khiem Pham\*, Tiancheng Yuan, Charlie Ruan, *Yucheng Lu*, Chris De Sa\n
/ In Proceedings of the 36th Neural Information Processing Systems Conference (NeurIPS) 2023. / \n
\[[https://openreview.net/forum?id=ISRyILhAyS Proceedings]\]\[[https://arxiv.org/abs/2302.00845 Arxiv]\]

- *CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks*\n
Jue Wang\*, *Yucheng Lu*\*, Binhang Yuan, Beidi Chen, Percy Liang, Chris De Sa, Chris RÃ©, Ce Zhang\n
/ In the Fortieth International Conference on Machine Learning (ICML) 2023. / \n
\[[https://openreview.net/pdf?id=w2Vrl0zlzA Proceedings]\]

- *STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition*\n
*Yucheng Lu*, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Chris De Sa, Amir Yazdanbakhsh\n
/ In the Fortieth International Conference on Machine Learning (ICML) 2023. / \n
\[[https://proceedings.mlr.press/v202/lu23c/lu23c.pdf Proceedings]\]\[[https://arxiv.org/abs/2302.01172 Arxiv]\]

- *Maximizing Communication Efficiency for Large-scale Training via 0\/1 Adam*\n
*Yucheng Lu*, Conglong Li, Minjia Zhang, Chris De Sa, Yuxiong He\n
/ In the Eleventh International Conference on Learning Representations (ICLR) 2023. / \n
\[[https://arxiv.org/abs/2202.06009 Arxiv]\]\[[https://www.deepspeed.ai/tutorials/zero-one-adam/ Tutorial]\]\[[https://github.com/microsoft/DeepSpeed Code]\]

- *Decentralized Learning: Theoretical Optimality and Practical Improvements*\n
*Yucheng Lu*, Chris De Sa\n
/ In the Journal of Machine Learning Research (JMLR) 2023. / \n
\[[https://www.jmlr.org/papers/volume24/22-0044/22-0044.pdf JMLR]\] \n


== 2022

- *GraB: Finding Provably Better Data Permutations than Random Reshuffling*\n
*Yucheng Lu*, Wentao Guo, Chris De Sa\n
/ In the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022. / \n
\[[https://openreview.net/pdf?id=nDemfqKHTpK Proceedings]\]\[[https://arxiv.org/abs/2205.10733 Arxiv]\]\[[https://github.com/EugeneLYC/GraB Code]\]

- *A General Analysis of Example-Selection for Stochastic Gradient Descent*\n
*Yucheng Lu*\*, Si Yi Meng\*, Chris De Sa\n
/ In the Tenth International Conference on Learning Representations (ICLR) 2022. / {{<strong style="color: red;">Spotlight</strong>}} \n
\[[https://openreview.net/pdf?id=7gWSJrP3opB Proceedings]\]\[[https://github.com/EugeneLYC/qmc-ordering Code]\]


== 2021

- *Hyperparameter Optimization is Deceiving Us, and How to Stop It*\n
A. Feder Cooper, *Yucheng Lu*, Jessica Zosa Forde, Chris De Sa\n
/ In the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) 2021. / \n
\[[https://proceedings.neurips.cc/paper/2021/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html Proceedings]\]\[[https://arxiv.org/abs/2102.03034 Arxiv]\]\[[https://github.com/pasta41/deception Code]\]

- *Variance Reduced Training with Stratified Sampling for Forecasting Models*\n
*Yucheng Lu*, Youngsuk Park, Lifan Chen, Yuyang Wang, Chris De Sa, Dean Foster\n
/ In the Thirty-eighth International Conference on Machine Learning (ICML) 2021. / \n
\[[http://proceedings.mlr.press/v139/lu21d.html Proceedings]\]\[[https://arxiv.org/abs/2103.02062 Arxiv]\]\[[https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/nursery Code]\]

- *Optimal Complexity in Decentralized Training*\n
*Yucheng Lu*, Chris De Sa\n
/ In the Thirty-eighth International Conference on Machine Learning (ICML) 2021. / {{<strong style="color: red;"> Outstanding Paper Award Honorable Mention</strong>}} \n
\[[http://proceedings.mlr.press/v139/lu21a.html Proceedings]\]\[[https://arxiv.org/abs/2006.08085 Arxiv]\]\[[./files/DeTAG_errata.pdf Errata]\]\[[https://www.leiphone.com/category/academic/ttPXXZRVE2IgyJfj.html Media Coverage (Chinese)]\] \n


== 2020

- *MixML: A Unified Analysis of Weakly Consistent Parallel Learning*\n
*Yucheng Lu*, Jack Nash, Chris De Sa\n
/ Unpublished Manuscript / \n
\[[https://arxiv.org/abs/2005.06706 Arxiv]\]

- *Adaptive Diffusion of Sensitive Information In Online Social Networks*\n
Xudong Wu, Luoyi Fu, Huan Long, Dali Yang, *Yucheng Lu*, Xinbing Wang, Guihai Chen\n
/ In IEEE Transactions on Knowledge and Data Engineering (TKDE) 2020. / \n
\[[https://ieeexplore.ieee.org/abstract/document/8950034 Paper]\]

- *Moniqua: Modulo Quantized Communication in Decentralized SGD*\n
*Yucheng Lu*, Chris De Sa\n
/ In the Thirty-seventh International Conference on Machine Learning (ICML) 2020. / \n
\[[http://proceedings.mlr.press/v119/lu20a.html Proceedings]\]\[[https://arxiv.org/abs/2002.11787 Arxiv]\]

